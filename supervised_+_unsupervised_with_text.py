# -*- coding: utf-8 -*-
"""Supervised + Unsupervised with Text

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TH89T1T3a3sx9pNmtBMk78dtqQxDystc
"""

import tarfile 
with tarfile.open ("/content/aclImdb_v1.tar.gz","r") as tar:
  def is_within_directory(directory, target):
      
      abs_directory = os.path.abspath(directory)
      abs_target = os.path.abspath(target)
  
      prefix = os.path.commonprefix([abs_directory, abs_target])
      
      return prefix == abs_directory
  
  def safe_extract(tar, path=".", members=None, *, numeric_owner=False):
  
      for member in tar.getmembers():
          member_path = os.path.join(path, member.name)
          if not is_within_directory(path, member_path):
              raise Exception("Attempted Path Traversal in Tar File")
  
      tar.extractall(path, members, numeric_owner=numeric_owner) 
      
  
  safe_extract(tar)

import pandas as pd
import os

df=pd.DataFrame()

basepath="aclImdb"
labels={"pos":1,"neg":0}

for s in ("test","train"):
  for l in ("pos","neg"):
    path=os.path.join(basepath,s,l)
    for file in sorted(os.listdir(path)):
      with open(os.path.join(path,file),"r",encoding="utf-8") as infile:
        txt=infile.read()
      df=df.append([[txt,labels[l]]],ignore_index=True)

df.columns=["Review","Sentiment"]

df

import numpy as np
np.random.seed(0)

df=df.reindex(np.random.permutation(df.index))

df=df.reset_index(drop=True)

df.head(5)

df.iloc[0,0]

import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
import re,string,unicodedata
from string import punctuation

nltk.download("stopwords")

stop=set(stopwords.words("english"))
punct=list(string.punctuation)
stop.update(punct)

def clean_html(text):
  soup=BeautifulSoup(text,"html.parser") 
  return soup.get_text()

def clean_url(text):
  return re.sub(r"http\S+","",text)

def clean_stopwords(text):
  final_text=[]  
  for i in text.split():
    if i.strip().lower() not in stop and i.strip().lower().isalpha():
      final_text.append(i.strip().lower())
  return " ".join(final_text)

df.iloc[25000,0]

from nltk.stem.porter import PorterStemmer
porter=PorterStemmer()
def stemmer(text):
  final_text=[porter.stem(word)for word in text.split()]
  return " ".join(final_text)

  #Stemmer return the root of the word. example houses->hous

def clean_text(text):
  text=clean_html(text)
  text=clean_url(text)
  text=clean_stopwords(text)
  text=stemmer(text)
  return text

df["Review"]=df["Review"].apply(clean_text)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

count=CountVectorizer(max_df=.1,max_features=5000,stop_words="english")

X=count.fit_transform(df["Review"].values)

lda=LatentDirichletAllocation(n_components=10,learning_method="batch",random_state=123)

#identify the probability that a word belongs to a specific topic

x_topics=lda.fit_transform(X)

max_words=10
feature_names=count.get_feature_names()
for idx,topic in enumerate(lda.components_):
  print(f"topico {idx+1}")
  print(" ".join([feature_names[i] for i in topic.argsort()[:-max_words-1:-1]]))

  #we do topic modelling to identify what commontopics we have in all the reviews.
  #In the total of all the reviews we have 10 topics(itÂ´s like doing clustering ) 
  #this is useful for a company that ahs thousands of emails to identify the topics why people is contacting them

#SENTIMENT ANALYSIS

xtrain=df.loc[:35000,"Review"].values
ytrain=df.loc[:35000,"Sentiment"].values

xtest=df.loc[35000:,"Review"].values
ytest=df.loc[35000:,"Sentiment"].values

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer()
#transform text into numbers 
lr_tfidf=Pipeline([("vect",tfidf),
                   ("clf",LogisticRegression(solver="liblinear",C=10,penalty="l2",random_state=42))])
#we do the LR model

lr_tfidf.fit(xtrain,ytrain)

lr_tfidf.score(xtest,ytest)

lr_tfidf.predict(xtest[:10])

#this the sentiment prediction using the model

ytest[:10]
#this is the Sentiment column, as we can see with what the model has predicted, we have only failed the last one

df.head(5)

xtrain

def prueba():
  review=input("Write your review: ")
  review=clean_text(review)
  review=np.array([review])
  sent=lr_tfidf.predict(review)[0]
  if sent==0:
    print("Negative")
  else:
    print("Positive")

prueba()









